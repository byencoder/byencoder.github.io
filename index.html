<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and
    Navigation</title>
  <meta name="description" content="">
  <meta name="keywords" content="BYE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title"
    content="BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Visual Language Maps for Robot Navigation">
  <!-- <meta property="og:image" content="https://vlmaps.github.io/static/images/cover_lady.png" /> -->
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1082" />
  <meta property="og:image:height" content="639" />
  <meta property="og:url" content="https://byencoder.github.io" />
  <meta property="og:description"
    content="Project page for BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation" />
  <meta name="twitter:title"
    content="BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation" />
  <meta name="twitter:description"
    content="Project page for BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation" />
  <meta name="twitter:image" content="https://byencoder.github.io/static/images/cover_lady.png" />

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-E0P19745XR"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-E0P19745XR');
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vlmaps_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TRWCBBLV" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">BYE: Build Your Encoder with One Sequence of Exploration Data for
              Long-Term Dynamic Scene Understanding</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a><sup>1</sup> and
              </span>
              <span class="author-block">
                <a href="http://www2.informatik.uni-freiburg.de/~yan/">Shengchao Yan</a><sup>1</sup> and
              </span>
              <span class="author-block">
                <a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Freiburg University,</span>
              <span class="author-block"><sup>2</sup>University of Technology Nuremberg</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="BYE_Build_Your_Encoder.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <!-- CoLab Link. -->
                <!-- <span class="link-block">
                  <a href="https://colab.research.google.com/drive/1xsH9Gr_O36sBZaoPNq1SmqgOOF12spV0?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/colab_icon.png" />
                    </span>
                    <span>CoLab</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src=""
                type="video/mp4">
      </video> -->

        <!-- <img src="static/images/long_term_dynamic.png" /> -->
        <img src="static/images/cover_lady.png" />

        <h2 class="subtitle has-text-centered">
          BYE enables object association in long-term dynamic scenes by training a per-scene encoder with one sequence
          of exploration data
        </h2>

        <video autoplay muted loop playsinline width="100%">
          <source src="static/images/bye.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Dynamic scene understanding remains a persistent
              challenge in robotic applications. Early dynamic mapping meth-
              ods focused on mitigating the negative influence of short-term
              dynamic objects on camera motion estimation by masking or
              tracking specific categories, which often fall short in adapting to
              long-term scene changes. Recent efforts address object associa-
              tion in long-term dynamic environments using neural networks
              trained on synthetic datasets, but they still rely on predefined
              object shapes and categories. Other methods incorporate visual,
              geometric, or semantic heuristics for the association but often lack
              robustness. In this work, we introduce BYE, a class-agnostic, per-
              scene point cloud encoder that removes the need for predefined
              categories, shape priors, or extensive association datasets. Trained
              on only a single sequence of exploration data, BYE can efficiently
              perform object association in dynamically changing scenes. We
              further propose an ensembling scheme combining the semantic
              strengths of Vision Language Models (VLMs) with the scene-
              specific expertise of BYE, achieving a 7% improvement and a
              95% success rate in object association tasks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/images/bye_ral_submission.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <img src="static/images/overview.png" />
            <h3 class="title is-4">Instance-Level Mapping</h3>
            <p>
              Given RGB, depth, odometry and instance masks of one sequence of exploration data, we build an
              instance-level map consisting of a list of point clouds where each point cloud represent an object in the
              scene.
            </p>
            <h3 class="title is-4">Partial Point Cloud Observation Dataset Generation</h3>
            <p>
              We label each back-projected instance mask associated with an object with its object ID in the
              instance-level map, forming a dataset consisting of samples like (partial observation, object ID).
            </p>
            <h3 class="title is-4">Train a Per-Scene Point Cloud Encoder with Contrastive Learning</h3>
            <p>
              We exploit the scheme of SimCLR to train a point cloud encoder to generate latent embeddings for point
              cloud. The embeddings of partial observations for the same object should be similar while those of
              different objects should be far away from one another in the embedding space. We use DGCNN or PointNet as
              the backbones of the point cloud encoder as is shown below.

            </p>
            <img src="static/images/architecture.png" />

            <h3 class="title is-4">Query with Voting using the Object Memory Bank</h3>
            <p>
              Given a new sequence of exploration data, we use the frozen encoder to encode the partial observations
              into embeddings, search k nearest neighbors and collect the neighbors' object IDs, which are used for
              voting for the associated reference object.
            </p>
            <img style="display: block; width: 70%; margin: 0 auto" src="static/images/query.png" />

            <h3 class="title is-4">Query with VLM Ensembling</h3>
            <p>
              To further improve the results, we propose an ensembling scheme combining the semantic strengths of VLMs
              and the scene-specific expertise of our BYE encoder. To start with, we first introduce the concept of
              association matrix and how to use it for object association. As is shown below, the row and column indices
              of an association matrix indicate the new and reference object IDs, respectively. The value at the (i, j)
              position of the matrix indicates the similarity between the i-th new object and the j-th reference object.
              We use the Hungarian Algorithm on the association matrix to assign a reference object ID for each new
              object ID.
            </p>
            <img style="display: block; width: 40%; margin: 0 auto" src="static/images/association_matrix.png" />

            <p>
              The key of the ensembling method is to derive the association matrices with both the BYE encoder and the
              VLM respectively, fuse them, and use the fused association matrix to predict association. The fused
              association matrix can be calculated by element-wise sum of the two association matrices.
            </p>
            <img style="display: block; width: 60%; margin: 0 auto"
              src="static/images/association_matrix_ensembling.png" />

            <p>
              The association matrix for BYE encoder can be calculated with the kNN results. The element at the (i, j)
              in the matrix can be calculated by dividing the count of reference object ID j in the kNN results of new
              object ID i by the total count of considered nearest neighbors. When we consider only a single frame, the
              denominator is K. When we take a sequence of observations, the denominator is number of frames times the
              K.
              The association matrix for VLM can be calculated by the
              cosine similarity between the embeddings of new and reference object IDs. The fused association matrix can
              be calculated by element-wise multiplication of the two association matrices.
            </p>
            <img style="display: block; width: 60%; margin: 0 auto" src="static/images/association_matrix_bye.png" />
            <p>
              The association matrix for VLM can be calculated by the cosine similarity between the embeddings of new
              and reference object.
            </p>
            <img style="display: block; width: 60%; margin: 0 auto" src="static/images/association_matrix_vlm.png" />


          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiment</h2>

                <!-- Interpolating. -->
                <h3 class="title is-4">Object Association in Long-Term Dynamic Environments</h3>
                <div class="content has-text-justified">

                  <img style="display: block; width: 50%; margin: 0 auto" src="static/images/long_term_dynamic.png" />
                  <p>
                    We collect exploration trials in 10 scenes (as is shown above), each with one reference trial and a
                    new trial of
                    exploration data. Then we need to find the mapping from the object ID in the new trial to the object
                    ID in the reference trial. We compare our method with heuristic methods based on foundation models
                    that can generate dense visual-language features. The results show that our ensemble method achieves
                    an almost-perfect 95% success rate, with a notable margin over the foundation models. The results
                    are shown in the table below.
                  </p>
                  <img src="static/images/results.png" />
                </div>
                <div class="content has-text-justified">
                  <p>
                    We further show the qualitative results of object association in long-term dynamic environments.
                    When we first look at objects that are commonly occurring in various datasets like "Garbage Can",
                    "Bowl", "Stool", "Bed", "Desk", and "Laptop", VLMs like CLIP, OVSeg and LSeg perform
                    well in the association tasks. The major reason behind this might be
                    that the pre-training and fine-tuning processes allow the model to learn reliable and robust
                    features for those categories. When there are no duplicate objects in the scene (like "Bed" and
                    "Desk"), the semantic features are representative for those objects and therefore help with
                    association. However, when we look at less common categories like "Cell Phone", "Credit Card",
                    "Key Chain", and "Pan", fine-tuned foundation models struggle to correctly associate them due to
                    their long-tailed characteristics in the dataset. When we look at small objects such as "Pen", even
                    CLIP fails to associate them correctly. However, the ensembling method integrates the scene-specific
                    expertise with VLM, making these detailed knowledge contribute to the performance. The benefits of
                    BYE emerge under these circumstances since BYE is trained on the reference exploration data,
                    which is not restricted by the data distribution that pre-trained models are accustomed to. BYE only
                    focuses on the geometric and visual characteristics of the objects in the scenes and learns to
                    differentiate them in the process of contrastive learning.
                  </p>
                  <img src="static/images/qualitative_results_simulator.png" />
                </div>
                <h3 class="title is-4">Real World Results</h3>

                <div class="content has-text-justified">
                  <img style="display: inline-block; width: 49%; margin: 0 auto" src="static/images/chair00.gif" />
                  <img style="display: inline-block; width: 49%; margin: 0 auto" src="static/images/chair01.gif" />
                  <p>
                    We further evaluate our method on real-world data collected in a dynamic environment. As is shown
                    above, the left sequence is reference sequence and the right one is the new observation after object
                    relocations happened.
                  </p>
                  <img style="display: block; width: 80%; margin: 0 auto" src="static/images/real_world_results.png" />
                  <p>
                    The quantitative results show the robustness and effectiveness of our ensembling method with a 100%
                    success rate in object association tasks.
                  </p>
                  <img src="static/images/qualitative_results_real_world_tab_1.png" />
                  <p>
                    In the tabletop scene, objects are moved around randomly and the robot needs to associate them
                    across different trials.
                    Both our method and CLIP works perfectly in this scenario.
                  </p>
                  <img src="static/images/qualitative_results_real_world_furn_2.png" />
                  <p>
                    In the furniture scene, we randomly put eight chairs across three different rooms (Furn 1, Furn 2,
                    Furn 4) and test the association methods. This scenario is challenging since it requires the model
                    to perform association for the semantically similar objects. Some chairs are identical except for
                    different objects placed on them, which poses a challenge for VLMs which usually struggle with
                    differentiating small differences among the same type of objects. In this case, our method still
                    robustly achieve a 100% association success rate, while other VLMs face different degrees of
                    failures.
                  </p>

                  <h3 class="title is-4">Runtime Analysis</h3>
                  <div class="content has-text-justified">
                    <img style="display: block; width: 80%; margin: 0 auto" src="static/images/runtime.png" />
                    <p>
                      BYE can run efficiently on a RTX 3060 Ti GPU with frequency of 88 samples per second when a batch
                      size of 32 is used, making the method compatible and affordable with real-time applications.
                    </p>
                  </div>
                  <!-- <div class="columns is-vcentered interpolation-panel">
                  <p>Sequence 1</p>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_1_v1.mp4" type="video/mp4">
                    </video>
                    <p>VLMaps</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_cow_1.mp4" type="video/mp4">
                    </video>
                    <p>CLIP on Wheels</p>
                  </div>
                </div>


                <div class="columns is-vcentered interpolation-panel">
                  <p>Sequence 2</p>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_2_v1.mp4" type="video/mp4">
                    </video>
                    <p>VLMaps</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_cow_2.mp4" type="video/mp4">
                    </video>
                    <p>CLIP on Wheels</p>
                  </div>
                </div> -->


                  <br />
                  <!--/ Interpolating. -->

                  <!-- Re-rendering. -->
                  <!-- <h3 class="title is-4">Multi-Embodiment Navigation</h3>
                <div class="content has-text-justified">
                  <p>
                    A VLMap can be shared among different robots and enables generation of obstacle maps for different
                    embodiments on-the-fly to improve navigation efficiency. For example, a LoCoBot (ground robot) has
                    to avoid sofa, tables, chairs and so on during planning while a drone can ignore them. Experiments
                    below show how a single VLMap representation in each scene can adapt to different embodiments
                    (by generating customized obstacle maps) and improve navigation efficiency.
                  </p>
                </div>

                <p>Move to the laptop and the box sequentially</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_2_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_2_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <p>Move to the window</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_3_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_3_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <p>Move to the television</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_4_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_4_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div> -->

                  <!-- <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
                  <!--/ Re-rendering. -->

                </div>
              </div>
              <!--/ Animation. -->


              <!-- Concurrent Work. -->
              <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
              <!--/ Concurrent Work. -->

            </div>
      </section>


      <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code> @inproceedings{huang23vlmaps,
          title={Visual Language Maps for Robot Navigation},
          author={Chenguang Huang and Oier Mees and Andy Zeng and Wolfram Burgard},
          booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
          year={2023},
          address = {London, UK}
          } </code></pre>
        </div>
      </section> -->


      <section class="section">
        <div class="container is-max-desktop">
          <h2 class="title is-3">People</h2>
          <div class="columns container">
            <div class="column has-text-centered profile">
              <a href="http://www2.informatik.uni-freiburg.de/~huang/"><img style="max-width: 450px; max-height: 200px"
                  src="static/images/huang.jpg" alt="Chenguang Huang" /></a>
              <h3><a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a></h3>
            </div>

            <div class="column has-text-centered profile">
              <a href="http://www2.informatik.uni-freiburg.de/~yan/"><img style="max-width: 450px; max-height: 200px"
                  src="static/images/yan.jpg" alt="Shengchao Yan" /></a>
              <h3><a href="http://www2.informatik.uni-freiburg.de/~yan/">Shengchao Yan</a></h3>
            </div>

            <div class="column has-text-centered profile">
              <a href="https://www.utn.de/1/wolfram-burgard/"><img style="max-width: 450px; max-height: 200px"
                  src="static/images/burgard.jpg" alt="Wolfram Burgard" /></a>
              <h3><a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a></h3>
            </div>

          </div>

        </div>
      </section>


      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <a class="icon-link" href="https://arxiv.org/pdf/2210.05714.pdf">
              <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                    International</a>
                </p>

              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>