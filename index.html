<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and
    Navigation</title>
  <meta name="description" content="">
  <meta name="keywords" content="BYE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title"
    content="BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Visual Language Maps for Robot Navigation">
  <!-- <meta property="og:image" content="https://vlmaps.github.io/static/images/cover_lady.png" /> -->
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1082" />
  <meta property="og:image:height" content="639" />
  <meta property="og:url" content="https://byencoder.github.io" />
  <meta property="og:description"
    content="Project page for BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation" />
  <meta name="twitter:title"
    content="BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation" />
  <meta name="twitter:description"
    content="Project page for BYE: Build Your Encoder with One Sequence of Exploration Data for Long-Term Dynamic Scene Understanding and Navigation" />
  <meta name="twitter:image" content="https://byencoder.github.io/static/images/cover_lady.png" />

  <!-- Google Tag Manager -->
  <!-- <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : "; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-PFJ2DFW');</script> -->
  <!-- End Google Tag Manager -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vlmaps_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFJ2DFW" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">BYE: Build Your Encoder with One Sequence of Exploration Data for
              Long-Term Dynamic Scene Understanding and Navigation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a><sup>1</sup> and
              </span>
              <span class="author-block">
                <a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Freiburg University,</span>
              <span class="author-block"><sup>2</sup>University of Technology Nuremberg</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="BYE_CoRL2024_lifelong_ws_final.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <!-- CoLab Link. -->
                <!-- <span class="link-block">
                  <a href="https://colab.research.google.com/drive/1xsH9Gr_O36sBZaoPNq1SmqgOOF12spV0?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/colab_icon.png" />
                    </span>
                    <span>CoLab</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src=""
                type="video/mp4">
      </video> -->

        <img src="static/images/long_term_dynamic.png" />

        <h2 class="subtitle has-text-centered">
          BYE enables object association in long-term dynamic scenes by training a per-scene encoder with one sequence
          of exploration data
        </h2>
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/back_and_forth_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move back and forth between the box and the keyboard</p>

          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/right_between_v3_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move right 1.5 meters, then move left 3 meters, then move left 1.5 meters</p>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/right_left_right_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move right 1.5 meters, then move left 3 meters, then move left 1.5 meters</p>
          </div>
          <div class="item item-shiba has-text-centered">
            <video poster="" id="shiba video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/move_to_plant_x8_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move to the plant</p>
          </div>
          <div class="item item-fullbody has-text-centered">
            <video poster="" id="fullbody video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/move_in_between_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move in between the wooden box and the chair</p>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Dynamic scene understanding has long been a challenge in robotic applications. Earlier approaches to
              dynamic mapping focused on mitigating the impact of short-term dynamic objects in view, typically by
              removing or tracking masks for specific object categories while estimating camera motion. However, these
              methods often struggle to handle long-term scene changes. Recent efforts have addressed the object
              association problem in long-term dynamic environments using neural networks trained on synthetic datasets,
              though these approaches still rely on predefined object shapes and categories. Other methods leverage
              visual, geometric, or semantic clues as heuristics for association. In this work, we introduce BYE, a
              class-agnostic per-scene point cloud encoder that eliminates the need for predefined categories, shape
              priors, or large association datasets. It requires only a single sequence of exploration data for training
              and can efficiently perform object association in the face of dynamic changes.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video autoplay muted loop playsinline width="100%">
              <source src="static/images/bye.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <img src="static/images/overview.png" />
            <h3 class="title is-4">Instance-Level Mapping</h3>
            <p>
              Given RGB, depth, odometry and instance masks of one sequence of exploration data, we build an
              instance-level map consisting of a list of point clouds where each point cloud represent an object in the
              scene.
            </p>
            <h3 class="title is-4">Partial Point Cloud Observation Dataset Generation</h3>
            <p>
              We label each back-projected instance mask associated with an object with its object ID in the
              instance-level map, forming a dataset consisting of samples like (partial observation, object ID).
            </p>
            <h3 class="title is-4">Train a Per-Scene Point Cloud Encoder with Contrastive Learning</h3>
            <p>
              We exploit the scheme of SimCLR to train a point cloud encoder to generate latent embeddings for point
              cloud. The embeddings of partial observations for the same object should be similar while those of
              different objects should be far away from one another in the embedding space. We use DGCNN as the backbone
              of the point cloud encoder as is shown below.

            </p>
            <img src="static/images/architecture.png" />

          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiment</h2>

                <!-- Interpolating. -->
                <h3 class="title is-4">Object Association in Long-Term Dynamic Environments</h3>
                <div class="content has-text-justified">
                  <img src="static/images/results.png" />
                  <p>
                    We collect exploration trials in 10 scenes, each with one reference trial and a new trial of
                    exploration data. Then we need to find the mapping from the object ID in the new trial to the object
                    ID in the reference trial. We compare our method with heuristic methods based on foundation models
                    that can generate dense visual-language features. The results show that our method achieves overall
                    better association success rate.
                  </p>
                </div>
                <div class="content has-text-justified">
                  <img src="static/images/per_clas_association.png" />
                  <p>
                    We further show the qualitative results of object association in long-term dynamic environments.
                    When we first look at objects that are commonly occurring in various datasets like "Garbage Can",
                    "Bowl", "Stool", "Bed", "Desk", and "Laptop", fine-tuned VLMs like OVSeg and LSeg perform
                    well in the association tasks, sometimes even better than BYE. The major reason behind this might be
                    that the pre-training and fine-tuning processes allow the model to learn reliable and robust
                    features for those categories. When there are no duplicate objects in the scene (like "Bed" and
                    "Desk"), the semantic features are representative for those objects and therefore help with
                    association. However, when we look at less common categories like "Cell Phone", "Credit Card",
                    "Key Chain", and "Pan", foundation models struggle to correctly associate them due to their
                    long-tailed characteristics in the dataset. The benefits of BYE emerge under these circumstances.
                    Since \ours{} is trained only on the reference exploration data, which is not restricted by the data
                    distribution that pre-trained models are accustomed to. \ours{} only focuses on the geometric and
                    visual characteristics of the objects in the scenes and learns to differentiate them in the process
                    of contrastive learning.
                  </p>
                </div>
                <h3 class="title is-4">Runtime Analysis</h3>
                <div class="content has-text-justified">
                  <img src="static/images/runtime.png" />
                  <p>
                    BYE can run at 50Hz on NVIDIA A40 GPU with a single CPU thread. The per-scene inference speed is
                    shown in the image above.
                  </p>
                </div>
                <!-- <div class="columns is-vcentered interpolation-panel">
                  <p>Sequence 1</p>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_1_v1.mp4" type="video/mp4">
                    </video>
                    <p>VLMaps</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_cow_1.mp4" type="video/mp4">
                    </video>
                    <p>CLIP on Wheels</p>
                  </div>
                </div>


                <div class="columns is-vcentered interpolation-panel">
                  <p>Sequence 2</p>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_2_v1.mp4" type="video/mp4">
                    </video>
                    <p>VLMaps</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_cow_2.mp4" type="video/mp4">
                    </video>
                    <p>CLIP on Wheels</p>
                  </div>
                </div> -->


                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <!-- <h3 class="title is-4">Multi-Embodiment Navigation</h3>
                <div class="content has-text-justified">
                  <p>
                    A VLMap can be shared among different robots and enables generation of obstacle maps for different
                    embodiments on-the-fly to improve navigation efficiency. For example, a LoCoBot (ground robot) has
                    to avoid sofa, tables, chairs and so on during planning while a drone can ignore them. Experiments
                    below show how a single VLMap representation in each scene can adapt to different embodiments
                    (by generating customized obstacle maps) and improve navigation efficiency.
                  </p>
                </div>

                <p>Move to the laptop and the box sequentially</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_2_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_2_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <p>Move to the window</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_3_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_3_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <p>Move to the television</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_4_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_4_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div> -->

                <!-- <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
                <!--/ Re-rendering. -->

              </div>
            </div>
            <!--/ Animation. -->


            <!-- Concurrent Work. -->
            <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
            <!--/ Concurrent Work. -->

          </div>
      </section>


      <!-- <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code> @inproceedings{huang23vlmaps,
          title={Visual Language Maps for Robot Navigation},
          author={Chenguang Huang and Oier Mees and Andy Zeng and Wolfram Burgard},
          booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
          year={2023},
          address = {London, UK}
          } </code></pre>
        </div>
      </section> -->


      <section class="section">
        <div class="container is-max-desktop">
          <h2 class="title is-3">People</h2>
          <div class="columns container">
            <div class="column has-text-centered profile">
              <a href="http://www2.informatik.uni-freiburg.de/~huang/"><img src="static/images/huang.jpg"
                  alt="Chenguang Huang" /></a>
              <h3><a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a></h3>
            </div>

            <div class="column has-text-centered profile">
              <a href="https://www.utn.de/1/wolfram-burgard/"><img src="static/images/burgard.jpg"
                  alt="Wolfram Burgard" /></a>
              <h3><a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a></h3>
            </div>

          </div>

        </div>
      </section>


      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <a class="icon-link" href="https://arxiv.org/pdf/2210.05714.pdf">
              <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                    International</a>
                </p>

              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>